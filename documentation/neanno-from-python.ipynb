{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using neanno from Python\n",
    "\n",
    "neanno has a couple of functions for working with annotations directly from Python code incl. predicting annotations. This notebook shows some of them. For an up-to-date view have a look at the `neanno.utils.text`, `neanno.utils.metrics` and `neanno.prediction.*` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure the directory where the neanno sources reside are in the path\n",
    "import os\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Load some data first\n",
    "\n",
    "To continue, we first need to load some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request_id</th>\n",
       "      <th>text</th>\n",
       "      <th>categories</th>\n",
       "      <th>is_text_annotated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2047</td>\n",
       "      <td>Hi all,\\n\\nI have booked to fly from `Sydney``...</td>\n",
       "      <td>Service Offering/Procedure|Technology</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1997</td>\n",
       "      <td>If my friend and I are turning 17, but want to...</td>\n",
       "      <td>Trip Planning|Customs/Immigration|Legal</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1999</td>\n",
       "      <td>Hey All,\\n\\nIn May, we'll be flying from `YYZ`...</td>\n",
       "      <td>Trip Planning|Security</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2003</td>\n",
       "      <td>Here is a little story for you `football``SK`´...</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1549</td>\n",
       "      <td>Quick question...\\n\\nI've just pre booked my `...</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   request_id                                               text  \\\n",
       "0        2047  Hi all,\\n\\nI have booked to fly from `Sydney``...   \n",
       "1        1997  If my friend and I are turning 17, but want to...   \n",
       "2        1999  Hey All,\\n\\nIn May, we'll be flying from `YYZ`...   \n",
       "3        2003  Here is a little story for you `football``SK`´...   \n",
       "4        1549  Quick question...\\n\\nI've just pre booked my `...   \n",
       "\n",
       "                                categories  is_text_annotated  \n",
       "0    Service Offering/Procedure|Technology               True  \n",
       "1  Trip Planning|Customs/Immigration|Legal               True  \n",
       "2                   Trip Planning|Security               True  \n",
       "3                                     None               True  \n",
       "4                                     None               True  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../samples/airline_tickets/texts.annotating.csv')\n",
    "df = df.fillna('None')\n",
    "df[[\"text\", \"categories\"]] = df[[\"text\", \"categories\"]].astype(str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting annotations\n",
    "\n",
    "Now that we loaded the data, we can have a look at what neanno provides.\n",
    "\n",
    "neanno gives you different ways to extract annotations from annotated texts. Base function for all these functions is the `extract_annotations_as_generator` function. It walks through the specified text and yields an annotation whenever encountered. \n",
    "\n",
    "Let's see the it in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All annotations from a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>type</th>\n",
       "      <th>entity_code</th>\n",
       "      <th>parent_terms</th>\n",
       "      <th>parent_terms_raw</th>\n",
       "      <th>start_net</th>\n",
       "      <th>end_net</th>\n",
       "      <th>start_gross</th>\n",
       "      <th>end_gross</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>parented_named_entity</td>\n",
       "      <td>FROM</td>\n",
       "      <td>SYD</td>\n",
       "      <td>SYD</td>\n",
       "      <td>35</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>parented_named_entity</td>\n",
       "      <td>TO</td>\n",
       "      <td>LAX</td>\n",
       "      <td>LAX</td>\n",
       "      <td>45</td>\n",
       "      <td>56</td>\n",
       "      <td>63</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>747-400</td>\n",
       "      <td>standalone_named_entity</td>\n",
       "      <td>AIRCRAFT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69</td>\n",
       "      <td>76</td>\n",
       "      <td>103</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>wifi</td>\n",
       "      <td>standalone_key_term</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>136</td>\n",
       "      <td>140</td>\n",
       "      <td>187</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>pay for</td>\n",
       "      <td>parented_key_term</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fees</td>\n",
       "      <td>fees</td>\n",
       "      <td>173</td>\n",
       "      <td>180</td>\n",
       "      <td>231</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          term                     type entity_code parent_terms  \\\n",
       "0       Sydney    parented_named_entity        FROM          SYD   \n",
       "1  Los Angeles    parented_named_entity          TO          LAX   \n",
       "2      747-400  standalone_named_entity    AIRCRAFT          NaN   \n",
       "3         wifi      standalone_key_term         NaN          NaN   \n",
       "4      pay for        parented_key_term         NaN         fees   \n",
       "\n",
       "  parent_terms_raw  start_net  end_net  start_gross  end_gross  \n",
       "0              SYD         35       41           35         59  \n",
       "1              LAX         45       56           63         90  \n",
       "2              NaN         69       76          103        127  \n",
       "3              NaN        136      140          187        198  \n",
       "4             fees        173      180          231        251  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from neanno.utils.text import extract_annotations_as_generator\n",
    "\n",
    "# get annotations\n",
    "first_text = df[\"text\"][0]\n",
    "annotations = extract_annotations_as_generator(first_text)\n",
    "\n",
    "# show (only first few to avoid blowing up the notebook)\n",
    "df_to_show = pd.DataFrame(annotations)\n",
    "df_to_show[[\n",
    "    \"term\",\n",
    "    \"type\",\n",
    "    \"entity_code\",\n",
    "    \"parent_terms\",\n",
    "    \"parent_terms_raw\",\n",
    "    \"start_net\",\n",
    "    \"end_net\",\n",
    "    \"start_gross\",\n",
    "    \"end_gross\"\n",
    "]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only the annotations of a certain type and entity code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>type</th>\n",
       "      <th>entity_code</th>\n",
       "      <th>parent_terms</th>\n",
       "      <th>parent_terms_raw</th>\n",
       "      <th>start_net</th>\n",
       "      <th>end_net</th>\n",
       "      <th>start_gross</th>\n",
       "      <th>end_gross</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>parented_named_entity</td>\n",
       "      <td>TO</td>\n",
       "      <td>LAX</td>\n",
       "      <td>LAX</td>\n",
       "      <td>45</td>\n",
       "      <td>56</td>\n",
       "      <td>63</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          term                   type entity_code parent_terms  \\\n",
       "0  Los Angeles  parented_named_entity          TO          LAX   \n",
       "\n",
       "  parent_terms_raw  start_net  end_net  start_gross  end_gross  \n",
       "0              LAX         45       56           63         90  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get annotations\n",
    "annotations = extract_annotations_as_generator(\n",
    "        first_text,\n",
    "        types_to_extract=[\"standalone_named_entity\", \"parented_named_entity\"],\n",
    "        entity_codes_to_extract=[\"TO\"]\n",
    "    )\n",
    "\n",
    "# show (only first few to avoid blowing up the notebook)\n",
    "df_to_show = pd.DataFrame(annotations)                        \n",
    "df_to_show[[\n",
    "    \"term\",\n",
    "    \"type\",\n",
    "    \"entity_code\",\n",
    "    \"parent_terms\",\n",
    "    \"parent_terms_raw\",\n",
    "    \"start_net\",\n",
    "    \"end_net\",\n",
    "    \"start_gross\",\n",
    "    \"end_gross\"\n",
    "]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing distributions\n",
    "There are also some functions to compute distributions, eg. the distribution of the categories, named entities or terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute and show the text categories distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>None</td>\n",
       "      <td>821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Service Offering/Procedure</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Trip Planning</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Technology</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Customs/Immigration</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Security</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Complaint</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Legal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Complaint/Feedback</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Frequency\n",
       "None                              821\n",
       "Service Offering/Procedure         27\n",
       "Trip Planning                      24\n",
       "Technology                          3\n",
       "Customs/Immigration                 3\n",
       "Security                            3\n",
       "Complaint                           2\n",
       "Legal                               1\n",
       "Complaint/Feedback                  1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from neanno.utils.text import compute_categories_distribution_from_column\n",
    "\n",
    "# get distribution\n",
    "categories_distribution = compute_categories_distribution_from_column(df[\"categories\"])\n",
    "\n",
    "# show\n",
    "df_to_show = pd.DataFrame.from_dict(categories_distribution, orient=\"index\")\n",
    "df_to_show.columns = [\"Frequency\"]\n",
    "df_to_show = df_to_show.sort_values(by=[\"Frequency\"], ascending=False)\n",
    "df_to_show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the named entities distribution is similar. See the `neanno.utils.text` module for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the dictionary / term distribution\n",
    "\n",
    "Note: Named entity terms are understood as compound words, hence they are extracted as single term in the dictionary. This should give a better quality than just extracting single words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>to</td>\n",
       "      <td>3350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>2765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>I</td>\n",
       "      <td>2193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>and</td>\n",
       "      <td>1730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>a</td>\n",
       "      <td>1593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Term  Frequency\n",
       "0   to       3350\n",
       "1  the       2765\n",
       "2    I       2193\n",
       "3  and       1730\n",
       "4    a       1593"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from neanno.utils.text import compute_term_distribution_from_column\n",
    "from operator import itemgetter\n",
    "\n",
    "# get term distribution\n",
    "term_distribution = compute_term_distribution_from_column(df[\"text\"], include_entity_codes=False)\n",
    "\n",
    "# show (only first few to avoid blowing up the notebook)\n",
    "df_to_show = pd.DataFrame(sorted(term_distribution.items(), key = itemgetter(1), reverse = True), columns=[\"Term\", \"Frequency\"])\n",
    "df_to_show.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "The evaluation metric computations can be found in `neanno.utils.metrics`.\n",
    "\n",
    "### Compute precision/recall for recognized named entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct</th>\n",
       "      <th>incorrect</th>\n",
       "      <th>number_predictions</th>\n",
       "      <th>possible</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>AIRCRAFT</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>AIRLINE</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>AT</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>FROM</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>TO</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>VIA</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          correct  incorrect  number_predictions  possible  precision  recall\n",
       "AIRCRAFT     13.0        0.0                13.0      13.0        1.0     1.0\n",
       "AIRLINE      52.0        0.0                52.0      52.0        1.0     1.0\n",
       "AT            7.0        0.0                 7.0       7.0        1.0     1.0\n",
       "FROM         44.0        0.0                44.0      44.0        1.0     1.0\n",
       "TO           64.0        0.0                64.0      64.0        1.0     1.0\n",
       "VIA          15.0        0.0                15.0      15.0        1.0     1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from neanno.utils.metrics import compute_ner_metrics\n",
    "\n",
    "# get metrics (using the same annotations for actual/predicted for the sake of simplicity)\n",
    "ner_metrics = compute_ner_metrics(df[\"text\"], df[\"text\"])\n",
    "\n",
    "# show\n",
    "df_to_show = pd.DataFrame(ner_metrics).T\n",
    "df_to_show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Predict annotations\n",
    "\n",
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'term': 'wifi', 'type': 'standalone_key_term', 'start_net': 10, 'end_net': 14, 'start_gross': 10, 'end_gross': 21}\n"
     ]
    }
   ],
   "source": [
    "from neanno.prediction.pipeline import PredictionPipeline\n",
    "from neanno.prediction.key_terms.from_dataset import FromDatasetKeyTermsPredictor\n",
    "\n",
    "# create a prediction pipeline\n",
    "prediction_pipeline = PredictionPipeline()\n",
    "\n",
    "# create and add a predictor to the pipeline\n",
    "# notes: - a pipeline can have an arbitrary number of predictors\n",
    "#        - see the sample project files and/or the validation schema within the\n",
    "#          predictor classes for more infos about the config options\n",
    "#        - predictors validate the config they are given during instantiation\n",
    "key_terms_predictor = FromDatasetKeyTermsPredictor({\n",
    "    \"location\": \"csv:../samples/airline_tickets/default.key_terms.csv\"\n",
    "})\n",
    "key_terms_predictor.load_dataset(\"csv:../samples/airline_tickets/default.key_terms.csv\")\n",
    "prediction_pipeline.add_predictor(key_terms_predictor)\n",
    "\n",
    "# ask the pipeline to predict some annotations from a text\n",
    "text_with_predicted_annotations = prediction_pipeline.predict_inline_annotations(\"Can I use wifi during flight?\")\n",
    "for annotation in extract_annotations_as_generator(text_with_predicted_annotations):\n",
    "    print(annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "#### Online Training with single cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'term': 'wifi', 'type': 'standalone_key_term', 'start_net': 10, 'end_net': 14, 'start_gross': 10, 'end_gross': 21}\n"
     ]
    }
   ],
   "source": [
    "# teach new annotations\n",
    "# note: when we teach a FromDatasetKeyTermsPredictor, it will write back its learnings to the dataset.\n",
    "#       to avoid breaking the key terms dataset of the airline_tickets sample, we simply teach a term\n",
    "#       which is already known.\n",
    "prediction_pipeline.learn_from_annotated_text(\"Can I use `wifi``SK`´ during flight?\", \"en-US\")\n",
    "\n",
    "# ask the pipeline again to predict the annotations\n",
    "# note: the language parameter is optional. if it is not specified, en-US will be used as default.\n",
    "text_with_predicted_annotations = prediction_pipeline.predict_inline_annotations(\"Can I use wifi during flight?\", \"en-US\")\n",
    "for annotation in extract_annotations_as_generator(text_with_predicted_annotations):\n",
    "    print(annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Training\n",
    "\n",
    "The `FromDatasetKeyTermsPredictor` (as example) is a predictor which learns from single text examples. There are however also predictors which learn from a dataset/in a batch, eg. the `FromSpacyNamedEntitiesPredictor`. To teach these predictors, you have to use the pipeline's `learn_from_annotated_dataset` method.\n",
    "\n",
    "> Note: It's important to know that predictors built for online training will not learn automatically if batch training is started (except the predictors support batch training as well). Use the right training method for each predictor. If things don't match, the respective predictor will learn nothing and no exception will be thrown!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 46 annotated texts for training/testing.\n",
      "Training NER model with predictor '7df45b33-f64e-411b-9cfd-ff10fdd844ea'...\n",
      "Start time: 15:16:50\n",
      "Iteration: 0...\n",
      "=> loss: 3379.497282475233\n",
      "Iteration: 1...\n",
      "=> loss: 288.8349625951413\n",
      "Iteration: 2...\n",
      "=> loss: 250.96018050980638\n",
      "Iteration: 3...\n",
      "=> loss: 225.5531679164199\n",
      "Iteration: 4...\n",
      "=> loss: 306.0986836869047\n",
      "Iteration: 5...\n",
      "=> loss: 187.08639842035768\n",
      "Iteration: 6...\n",
      "=> loss: 154.7044283660357\n",
      "Iteration: 7...\n",
      "=> loss: 132.14694662487435\n",
      "Iteration: 8...\n",
      "=> loss: 116.91393806953812\n",
      "Iteration: 9...\n",
      "=> loss: 100.45028124102333\n",
      "Iteration: 10...\n",
      "=> loss: 94.02794306172382\n",
      "Iteration: 11...\n",
      "=> loss: 82.95874312420742\n",
      "Iteration: 12...\n",
      "=> loss: 73.21186220398444\n",
      "Iteration: 13...\n",
      "=> loss: 78.98121153774079\n",
      "Iteration: 14...\n",
      "=> loss: 54.44498389060924\n",
      "Iteration: 15...\n",
      "=> loss: 48.729078348055495\n",
      "Iteration: 16...\n",
      "=> loss: 48.6481156074325\n",
      "Iteration: 17...\n",
      "=> loss: 44.66407203047881\n",
      "Iteration: 18...\n",
      "=> loss: 82.81561555325541\n",
      "Iteration: 19...\n",
      "=> loss: 289.18575028158864\n",
      "Iteration: 20...\n",
      "=> loss: 40.62488969160707\n",
      "Iteration: 21...\n",
      "=> loss: 35.564916468533575\n",
      "Iteration: 22...\n",
      "=> loss: 33.489509086116854\n",
      "Iteration: 23...\n",
      "=> loss: 35.94832618685736\n",
      "Iteration: 24...\n",
      "=> loss: 28.46432240034764\n",
      "Iteration: 25...\n",
      "=> loss: 19.73419945724668\n",
      "Iteration: 26...\n",
      "=> loss: 30.072683855103094\n",
      "Iteration: 27...\n",
      "=> loss: 40.547162915030846\n",
      "Iteration: 28...\n",
      "=> loss: 16.15289488228878\n",
      "Iteration: 29...\n",
      "=> loss: 27.781231823250994\n",
      "Computing precision/recall matrix...\n",
      "         correct  incorrect  number_predictions  possible  precision    recall\n",
      "FROM         5.0        1.0                 6.0       7.0   0.833333  0.714286\n",
      "TO           3.0        1.0                 4.0      11.0   0.750000  0.272727\n",
      "AIRLINE      7.0        6.0                13.0       8.0   0.538462  0.875000\n",
      "End time: 15:17:16\n",
      "Training took (hh:mm:ss): 00:00:26.\n",
      "=> Success\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from neanno.prediction.named_entities.from_spacy import FromSpacyNamedEntitiesPredictor\n",
    "from neanno.configuration.definitions import NamedEntityDefinition\n",
    "\n",
    "# create a FromSpacyNamedEntitiesPredictor and add it to the pipeline\n",
    "named_entities_predictor = FromSpacyNamedEntitiesPredictor({\n",
    "      \"source_model\": \"blank:en\"\n",
    "    }\n",
    ")\n",
    "prediction_pipeline.add_predictor(named_entities_predictor)\n",
    "\n",
    "# disable the key terms predictor just for fun - because we can ;-)\n",
    "key_terms_predictor.is_prediction_enabled = False\n",
    "\n",
    "# show many annotated texts we have currently\n",
    "print(\"Using {} annotated texts for training/testing.\".format(df[\"is_text_annotated\"].sum()))\n",
    "\n",
    "# ask the pipeline to learn from the dataset\n",
    "language_column = \"\"\n",
    "categories_column = \"\"\n",
    "categories_to_train = []\n",
    "entity_codes_to_train = [\"FROM\", \"TO\", \"AIRLINE\"]\n",
    "prediction_pipeline.learn_from_annotated_dataset(df, \"text\", \"is_text_annotated\",\n",
    "    language_column, categories_column, categories_to_train, entity_codes_to_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cancun = TO\n",
      "Contoso = FROM\n",
      "\n",
      "Germany = TO\n"
     ]
    }
   ],
   "source": [
    "# ask the pipeline again to predict some inline annotations\n",
    "text_with_predicted_annotations = prediction_pipeline.predict_inline_annotations(\"We wanna fly to Cancun next year with Contoso.\")\n",
    "for annotation in extract_annotations_as_generator(text_with_predicted_annotations):\n",
    "    print(\"{} = {}\".format(annotation[\"term\"], annotation[\"entity_code\"]))\n",
    "    \n",
    "print(\"\")\n",
    "\n",
    "text_with_predicted_annotations = prediction_pipeline.predict_inline_annotations(\"I went to Germany last summer with Lufthansa.\")\n",
    "for annotation in extract_annotations_as_generator(text_with_predicted_annotations):\n",
    "    print(\"{} = {}\".format(annotation[\"term\"], annotation[\"entity_code\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring your own predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To bring your own predictor, you need to write a class which inherits from `neanno.prediction.predictor.Predictor` and either reference that new class in a neanno configuration file or use it directly in your Python code (see above).\n",
    "\n",
    "The included predictors are good templates to write your own predictors, eg. the `FromRegexesKeyTermsPredictor`.\n",
    "\n",
    "Depending on what your predictor shall do, there are different methods to implement. Mainly:\n",
    "\n",
    "- learn_from_annotated_text()\n",
    "- learn_from_annotated_dataset()\n",
    "- predict_inline_annotations()\n",
    "- predict_text_categories()\n",
    "\n",
    "Since the base class implements default variants of these methods already, new predictors have to implement these only if the predictor does something different than the base class. To see all methods you could inherit, see the above mentioned `Predictor` base class.\n",
    "\n",
    "When a predictor is created, it is passed a configuration, and the base class will then check if the configuration matches an expected schema. Predictors tell neanno which (additional to base class) configuration they expect by having the `project_config_validation_schema_custom_part` return a validation schema. neanno uses the cerberus package for validation. See the site [here](http://docs.python-cerberus.org/en/stable) for documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
